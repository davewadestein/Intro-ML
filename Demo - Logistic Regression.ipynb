{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Logistic Regression\n",
    "* source: https://github.com/Tebs-Lab/data-science-three-day-workshop/blob/main/02-linear-logistic-regression/03-logistic-regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's look at a sigmoid function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# np.exp is Euler's constant, the base of the natural logarithm \n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "# Construct a range to plot\n",
    "r = np.linspace(-10, 10, 100)\n",
    "plt.plot(r, sigmoid(r), 'b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are the coefficients?\n",
    "\n",
    "* In linear regression we had a familiar function of a line with the formula:\n",
    "\n",
    "    $y = mx + b$\n",
    "<br>\n",
    "* ...if we had many features, we'd have a unique $m$ for each feature\n",
    "* With logistic regression we still have that same effect, where each feature has its own coefficent and we have an intercept as well–but, they all now appear as part of the exponent on the bottom of the division\n",
    "\n",
    "    $y = \\frac{1}{(1 + e^-(b + m_1x_1 + m_2x_2 + ... + m_nx_n))}$\n",
    "\n",
    "* And, instead of $y$ we really predict something we intepret as the probability that the datapoint is a 1\n",
    "* Often we express this in the equation using $p$ instead of $y$\n",
    "\n",
    "    $p = \\frac{1}{(1 + e^-(b_0 + b_1x_1 + b_2x_2 + ... + b_px_p))}$\n",
    "\n",
    "## How do these impact the function shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_two(x, m, b):\n",
    "    return 1 / (1 + np.exp(-m*x + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 100 linearly-spaced points from -10 to +10\n",
    "r = np.linspace(-10, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficient modifies the steepness of the curve\n",
    "plt.plot(r, sigmoid_two(r, .3, 0), 'r')\n",
    "plt.plot(r, sigmoid_two(r, 1, 0), 'b')\n",
    "plt.plot(r, sigmoid_two(r, 2, 0), 'y')\n",
    "plt.plot(r, sigmoid_two(r, 3, 0), 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intercept modifies the \"balancing point\" of the curve\n",
    "# by shifting it left and right\n",
    "plt.plot(r, sigmoid_two(r, 1, 0), 'b')\n",
    "plt.plot(r, sigmoid_two(r, 1, 1), 'g')\n",
    "plt.plot(r, sigmoid_two(r, 1, -1), 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the data?\n",
    "\n",
    "* Since the data is all labeled either 0 or 1 we end up with a binary classifier\n",
    "\n",
    "### Lets make some linear data again...\n",
    "\n",
    "* This time let's give the data labels of 0 or 1 based on their magnitude\n",
    "  * This simple relationship is ideal for logistic regression, but you'll find it works well even with less clear boundaries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(12)\n",
    "x = 10 * rng.rand(50) \n",
    "y = x > 5\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with a manually chosen sigmoid overlayed...\n",
    "r = np.linspace(0, 10, 100)\n",
    "plt.scatter(x, y);\n",
    "plt.plot(r, sigmoid_two(r, 1, 5), 'b')\n",
    "plt.axvline(x=5, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "The red line denotes the \"decision boundary\" which occurs at the inflection point on the sigmoid function: anything to the right of that line gets classified as a __`1`__ whatever that means in our dataset, to the left gets classified as a __`0`__. The actual __`y`__ position along the sigmoid represents our confidence in that prediction—the higher up and further right the more likely we think this datapoint is a __`1`__, and the further down/left the more likely we think it's a __`0`__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's actually fit a model and see what it comes up with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Once again, transform our vector...\n",
    "column_x = x[:, np.newaxis]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(column_x, y)\n",
    "\n",
    "print(model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.linspace(0, 10, 100)\n",
    "plt.scatter(x, y);\n",
    "plt.plot(r, sigmoid_two(r, model.coef_[0][0], -model.intercept_[0]), 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Classes vs. Binary Classification\n",
    "\n",
    "* There are two common ways to perform multi-class classification with logistic regression\n",
    "  * The one we'll consider is a strategy called \"One-vs-All\" and it's a simple extension of the basic idea–Create a unique sigmoid for every class represented in the dataset\n",
    "\n",
    "* e.g., if we have 3 classes (cat, dog, and frog) we'll create and fit three sigmoids such that we have:\n",
    "  * One classifying \"cat or not cat\"\n",
    "  * One classifying \"dog or not dog\"\n",
    "  * One classifying \"frog or not frog\"\n",
    "* When fitting the data, all non-cat datapoints will be treated as not cats when fitting the first function, etc.\n",
    "* To make a prediction we run our data through all three functions and the one with the highest probability is chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
